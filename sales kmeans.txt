
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#Importing the required libraries.


from sklearn.cluster import KMeans, k_means #For clustering
from sklearn.decomposition import PCA #Linear Dimensionality reduction.


df = pd.read_csv("sales_data_sample.csv", encoding = 'unicode_escape') #Loading the dataset.


df.isnull().sum()



df_drop  = ['ADDRESSLINE1', 'ADDRESSLINE2', 'STATUS','POSTALCODE', 'CITY', 'TERRITORY', 'PHONE', 'STATE', 'CONTACTFIRSTNAME', 'CONTACTLASTNAME', 'CUSTOMERNAME', 'ORDERNUMBER']
df = df.drop(df_drop, axis=1) #Dropping the categorical uneccessary columns along with columns having null values. Can't fill the null values are there are alot of null values.



df.isnull().sum()


df.head()




productline = pd.get_dummies(df['PRODUCTLINE'], dtype = int) #Converting the categorical columns. 
Dealsize = pd.get_dummies(df['DEALSIZE'], dtype = int)
df = pd.concat([df,productline,Dealsize], axis = 1)


df = df.drop(['COUNTRY','PRODUCTLINE','DEALSIZE', 'ORDERDATE'], axis=1)



df.head()



df['PRODUCTCODE'] = pd.Categorical(df['PRODUCTCODE']).codes #Converting the datatype.




df.head()


# ## Plotting the Elbow Plot to determine the number of clusters. 



distortions = [] # Within Cluster Sum of Squares from the centroid
K = range(1,10)
for k in K:
    kmeanModel = KMeans(n_clusters=k)
    kmeanModel.fit(df)
    distortions.append(kmeanModel.inertia_)   #Appeding the intertia to the Distortions 


# plt.figure(figsize=(16,8))
# plt.plot(K, distortions, 'bx-')
# plt.xlabel('k')
# plt.ylabel('Distortion')
# plt.title('The Elbow Method showing the optimal k')
# plt.show()

# ### As the number of k increases Inertia decreases.
# ### Observations: A Elbow can be observed at 3 and after that the curve decreases gradually.


X_train = df.values


model = KMeans(n_clusters = 3, random_state = 2)
model = model.fit(X_train)
pred = model.predict(X_train)



unique, counts = np.unique(pred, return_counts = True)
counts = counts.reshape(1,3)
counts_df = pd.DataFrame(counts, columns = ['C1', 'C2', 'C3'])


counts_df.head()


# ### Visulaization


pca = PCA(n_components=2) #Converting all the features into 2 columns to make it easy to visualize using Principal COmponent Analysis.



X_train = pca.fit_transform(X_train)
reduced_x = pd.DataFrame(X_train,columns=['PCA1','PCA2']) #Creating a DataFrame.
reduced_X['Clusters'] = pred #Adding the Clusters to the reduced dataframe.
reduced_x



model.cluster_centers_ #Finding the centriods. (3 Centriods in total. Each Array contains a centroids for particular feature )



reduced_centers = pca.transform(model.cluster_centers_) #Transforming the centroids into 3 in x and y coordinates


#Plotting the clusters 
plt.figure(figsize=(14,10))
#taking the cluster number and first column           
#taking the same cluster number and second column      
# Assigning the color
plt.scatter(reduced_X[reduced_X['Clusters'] == 0].loc[:,'PCA1'],reduced_X[reduced_X['Clusters'] == 0].loc[:,'PCA2'],color='slateblue')
plt.scatter(reduced_X[reduced_X['Clusters'] == 1].loc[:,'PCA1'],reduced_X[reduced_X['Clusters'] == 1].loc[:,'PCA2'],color='springgreen')
plt.scatter(reduced_X[reduced_X['Clusters'] == 2].loc[:,'PCA1'],reduced_X[reduced_X['Clusters'] == 2].loc[:,'PCA2'],color='indigo')


plt.scatter(reduced_centers[:,0],reduced_centers[:,1],color='black',marker='x',s=300)
